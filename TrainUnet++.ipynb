{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qMrCzY5UFSw"
      },
      "source": [
        "# Train U-Net ++ with MoNuSeg dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2lxyy4JgQB_"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkvEVj1DgUwU"
      },
      "outputs": [],
      "source": [
        "!cat /proc/cpuinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNP2A-Dcjc6O"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcpa5mbeK3dk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7guJCJRsK7MW"
      },
      "outputs": [],
      "source": [
        "%cd drive/MyDrive/nuclei_segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRuQXwqXJhHJ"
      },
      "outputs": [],
      "source": [
        "# https://github.com/dovahcrow/patchify.py\n",
        "!pip install patchify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0SwqgoBK0GL"
      },
      "source": [
        "## Make validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LKl6qvnLM4K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMrJsiMDLcpA"
      },
      "outputs": [],
      "source": [
        "def create_path(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWvuYRd2LPhn"
      },
      "outputs": [],
      "source": [
        "train_dir = \"dataset/monuseg/stain_normalized/train\"\n",
        "val_dir = \"dataset/monuseg/stain_normalized/validation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpCzs_rmLdeQ"
      },
      "outputs": [],
      "source": [
        "def make_validation_set(val_dir, train_dir, val_size=6, seed=42, fold=None):\n",
        "\n",
        "    create_path(val_dir)\n",
        "    create_path(os.path.join(val_dir, \"tissue_images\"))\n",
        "    create_path(os.path.join(val_dir, \"binary_masks\"))\n",
        "    create_path(os.path.join(val_dir, \"instance_masks\"))\n",
        "    create_path(os.path.join(val_dir, \"modified_masks\"))\n",
        "    \n",
        "    for j in sorted(glob.glob(os.path.join(val_dir, \"tissue_images\", \"*\"))):\n",
        "        try:\n",
        "            shutil.move(j, os.path.join(train_dir, \"tissue_images\"))\n",
        "            shutil.move(j.replace(\"tissue_images\", \"binary_masks\").replace(\"tif\", \"png\"), \n",
        "                        os.path.join(train_dir, \"binary_masks\"))\n",
        "            shutil.move(j.replace(\"tissue_images\", \"instance_masks\").replace(\"tif\", \"npy\"), \n",
        "                        os.path.join(train_dir, \"instance_masks\"))\n",
        "            shutil.move(j.replace(\"tissue_images\", \"modified_masks\").replace(\"tif\", \"png\"), \n",
        "                        os.path.join(train_dir, \"modified_masks\"))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    images_lst = sorted(glob.glob(os.path.join(train_dir, \"tissue_images\", \"*\")))\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(images_lst)\n",
        "    if fold is None:\n",
        "        random.seed(seed)\n",
        "        val_lst = random.sample(images_lst, val_size)\n",
        "    else:\n",
        "        val_lst = images_lst[(fold*val_size)-val_size: fold*val_size]\n",
        "        \n",
        "\n",
        "    for i in val_lst:\n",
        "        shutil.move(i, os.path.join(val_dir, \"tissue_images\"))\n",
        "        shutil.move(i.replace(\"tissue_images\", \"binary_masks\").replace(\"tif\", \"png\"), \n",
        "                    os.path.join(val_dir, \"binary_masks\"))\n",
        "        shutil.move(i.replace(\"tissue_images\", \"instance_masks\").replace(\"tif\", \"npy\"), \n",
        "                    os.path.join(val_dir, \"instance_masks\"))\n",
        "        shutil.move(i.replace(\"tissue_images\", \"modified_masks\").replace(\"tif\", \"png\"), \n",
        "                    os.path.join(val_dir, \"modified_masks\"))\n",
        "        \n",
        "    print(f\"Validation list: {[os.path.basename(i) for i in val_lst]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e_pPZeWOXFA"
      },
      "outputs": [],
      "source": [
        "make_validation_set(val_dir, train_dir, val_size=6, fold=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spZeuYqGPbOR"
      },
      "source": [
        "## read the tissue images & GTs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N63nSLT5O6za"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from patchify import patchify, unpatchify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJbxgUwmj0-Z"
      },
      "outputs": [],
      "source": [
        "print(cv2.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfyJ4CLUPBeV"
      },
      "outputs": [],
      "source": [
        "train_dir = \"dataset/monuseg/stain_normalized/train/tissue_images\"\n",
        "train_maskdir = \"dataset/monuseg/stain_normalized/train/binary_masks\"\n",
        "train_mask2dir = \"dataset/monuseg/stain_normalized/train/modified_masks\"\n",
        "\n",
        "val_dir   = \"dataset/monuseg/stain_normalized/validation/tissue_images\"\n",
        "val_maskdir = \"dataset/monuseg/stain_normalized/validation/binary_masks\"\n",
        "val_mask2dir = \"dataset/monuseg/stain_normalized/validation/modified_masks\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUDndE1dPV_c"
      },
      "outputs": [],
      "source": [
        "W = 1024\n",
        "H = 1024\n",
        "\n",
        "patch_size = (256, 256, 3)\n",
        "all_img_patches = []\n",
        "for x in tqdm(sorted(glob.glob(os.path.join(train_dir, \"*\"))), total=len(os.listdir(train_dir))):\n",
        "    single_img = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "    single_img = cv2.cvtColor(single_img, cv2.COLOR_BGR2RGB)\n",
        "    single_img = cv2.resize(single_img, (W, H), interpolation=cv2.INTER_LINEAR)\n",
        "    # patchify\n",
        "    single_img_patches = patchify(single_img, patch_size=patch_size, step=128)\n",
        "    # squeeze\n",
        "    single_img_patches = np.squeeze(single_img_patches)\n",
        "\n",
        "    for i in range(single_img_patches.shape[0]):\n",
        "        for j in range(single_img_patches.shape[1]):\n",
        "            all_img_patches.append(single_img_patches[i, j])\n",
        "    \n",
        "train_images = np.array(all_img_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2gfdNtxPkhw"
      },
      "outputs": [],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyCid9IcPlyK"
      },
      "outputs": [],
      "source": [
        "all_img_patches = []\n",
        "for x in tqdm(sorted(glob.glob(os.path.join(val_dir, \"*\"))), total=len(os.listdir(val_dir))):\n",
        "    single_img = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "    single_img = cv2.cvtColor(single_img, cv2.COLOR_BGR2RGB)\n",
        "    single_img = cv2.resize(single_img, (W, H), interpolation=cv2.INTER_LINEAR)\n",
        "    # patchify\n",
        "    single_img_patches = patchify(single_img, patch_size=patch_size, step=128)\n",
        "    # squeeze\n",
        "    single_img_patches = np.squeeze(single_img_patches)\n",
        "\n",
        "    for i in range(single_img_patches.shape[0]):\n",
        "        for j in range(single_img_patches.shape[1]):\n",
        "            all_img_patches.append(single_img_patches[i, j])\n",
        "\n",
        "val_images = np.array(all_img_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXX6QqKAP4Zi"
      },
      "outputs": [],
      "source": [
        "val_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVsn7JEIP5cI"
      },
      "outputs": [],
      "source": [
        "all_mask_patches = []\n",
        "for x in tqdm(sorted(glob.glob(os.path.join(train_maskdir, \"*\"))), total=len(os.listdir(train_maskdir))):\n",
        "    single_mask = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n",
        "    single_mask = cv2.resize(single_mask, (W, H), interpolation=cv2.INTER_NEAREST)\n",
        "    # patchify\n",
        "    single_mask_patches = patchify(single_mask, patch_size=(256, 256), step=128)\n",
        "    # squeeze\n",
        "    single_mask_patches = np.squeeze(single_mask_patches)\n",
        "\n",
        "    for i in range(single_mask_patches.shape[0]):\n",
        "        for j in range(single_mask_patches.shape[1]):\n",
        "            all_mask_patches.append(single_mask_patches[i, j])\n",
        "\n",
        "train_masks = np.array(all_mask_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43ERJyE_QH1V"
      },
      "outputs": [],
      "source": [
        "train_masks.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkOkN8NIQI5W"
      },
      "outputs": [],
      "source": [
        "all_mask_patches = []\n",
        "for x in tqdm(sorted(glob.glob(os.path.join(val_maskdir, \"*\"))), total=len(os.listdir(val_maskdir))):\n",
        "    single_mask = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n",
        "    single_mask = cv2.resize(single_mask, (W, H), interpolation=cv2.INTER_NEAREST)\n",
        "    # patchify\n",
        "    single_mask_patches = patchify(single_mask, patch_size=(256, 256), step=128)\n",
        "    # squeeze\n",
        "    single_mask_patches = np.squeeze(single_mask_patches)\n",
        "\n",
        "    for i in range(single_mask_patches.shape[0]):\n",
        "        for j in range(single_mask_patches.shape[1]):\n",
        "            all_mask_patches.append(single_mask_patches[i, j])\n",
        "\n",
        "val_masks = np.array(all_mask_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILL2tG8pQMD3"
      },
      "outputs": [],
      "source": [
        "val_masks.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-jFoL9jQM4l"
      },
      "outputs": [],
      "source": [
        "all_mask_patches = []\n",
        "for x in tqdm(sorted(glob.glob(os.path.join(train_mask2dir, \"*\"))), total=len(os.listdir(train_mask2dir))):\n",
        "    single_mask = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n",
        "    single_mask = cv2.resize(single_mask, (W, H), interpolation=cv2.INTER_NEAREST)\n",
        "    # patchify\n",
        "    single_mask_patches = patchify(single_mask, patch_size=(256, 256), step=128)\n",
        "    # squeeze\n",
        "    single_mask_patches = np.squeeze(single_mask_patches)\n",
        "\n",
        "    for i in range(single_mask_patches.shape[0]):\n",
        "        for j in range(single_mask_patches.shape[1]):\n",
        "            # mask_with_boarders = generate_boarder(single_mask_patches[i, j])\n",
        "            all_mask_patches.append(single_mask_patches[i, j])\n",
        "\n",
        "train_masks2 = np.array(all_mask_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrBDQrF3QPwd"
      },
      "outputs": [],
      "source": [
        "train_masks2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIgmGmtcQT1u"
      },
      "outputs": [],
      "source": [
        "all_mask_patches = []\n",
        "for x in tqdm(sorted(glob.glob(os.path.join(val_mask2dir, \"*\"))), total=len(os.listdir(val_mask2dir))):\n",
        "    single_mask = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n",
        "    single_mask = cv2.resize(single_mask, (W, H), interpolation=cv2.INTER_NEAREST)\n",
        "    # patchify\n",
        "    single_mask_patches = patchify(single_mask, patch_size=(256, 256), step=128)\n",
        "    # squeeze\n",
        "    single_mask_patches = np.squeeze(single_mask_patches)\n",
        "\n",
        "    for i in range(single_mask_patches.shape[0]):\n",
        "        for j in range(single_mask_patches.shape[1]):\n",
        "            # mask_with_boarders = generate_boarder(single_mask_patches[i, j])\n",
        "            all_mask_patches.append(single_mask_patches[i, j])\n",
        "\n",
        "val_masks2 = np.array(all_mask_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUSpkjxnQVzm"
      },
      "outputs": [],
      "source": [
        "val_masks2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Au_Cw7npQX-p"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "rnd = np.random.randint(len(train_images))\n",
        "# rnd = 222\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize=(12, 6))\n",
        "[axi.set_axis_off() for axi in ax.ravel()]\n",
        "\n",
        "ax[0].imshow(train_images[rnd])\n",
        "ax[0].set_title(\"Tissue Image\")\n",
        "\n",
        "ax[1].imshow(train_masks[rnd])\n",
        "ax[1].set_title(\"Mask\")\n",
        "\n",
        "# ax[2].imshow(train_images[rnd])\n",
        "ax[2].imshow(train_masks2[rnd])\n",
        "ax[2].set_title(\"Mask2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn5BQJI4QYuf"
      },
      "source": [
        "## One-hot encoding the modified masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVRjk1hjQjcw"
      },
      "outputs": [],
      "source": [
        "# train masks\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "n, h, w = train_masks.shape\n",
        "train_masks_reshaped = train_masks2.reshape(-1,)\n",
        "train_masks_reshaped_encoded = label_encoder.fit_transform(train_masks_reshaped)\n",
        "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
        "\n",
        "n_classes = 3\n",
        "train_masks_cat = to_categorical(train_masks_encoded_original_shape, num_classes=n_classes)\n",
        "train_masks_cat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09zsQ6T2QrDS"
      },
      "outputs": [],
      "source": [
        "# val masks\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "n, h, w = val_masks.shape\n",
        "val_masks_reshaped = val_masks2.reshape(-1,)\n",
        "val_masks_reshaped_encoded = label_encoder.fit_transform(val_masks_reshaped)\n",
        "val_masks_encoded_original_shape = val_masks_reshaped_encoded.reshape(n, h, w)\n",
        "\n",
        "n_classes = 3\n",
        "val_masks_cat = to_categorical(val_masks_encoded_original_shape, num_classes=n_classes)\n",
        "val_masks_cat.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVBMAy7EREoG"
      },
      "source": [
        "## Data augmentation using albumentations library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yUdZajtQvwz"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from keras.utils import Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_t-im4zkMDt"
      },
      "outputs": [],
      "source": [
        "print(A.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLKh8vZHRLpf"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, images, masks, masks_cat, augmentations=None, batch_size=8, img_size=256, n_channels=3, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "        self.masks_cat = masks_cat\n",
        "\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.n_channels = n_channels\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augmentations\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.ceil(len(self.images) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indices of the batch\n",
        "        indices = self.indices[index * self.batch_size: min((index + 1) * self.batch_size, len(self.images))]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.data_generation(indices)\n",
        "        y1 = y[0]\n",
        "        y2 = y[1]\n",
        "\n",
        "        if self.augment is None:\n",
        "            return X, [np.array(y1), np.array(y2)]\n",
        "        else:            \n",
        "            im, mask1, mask2 = [], [], []   \n",
        "            for x, y1, y2 in zip(X, y1, y2):\n",
        "                augmented = self.augment(image=x, mask1=y1, mask2=y2)\n",
        "                im.append(augmented['image'])\n",
        "                mask1.append(augmented['mask1'])\n",
        "                mask2.append(augmented['mask2'])\n",
        "\n",
        "            return np.array(im), [np.array(mask1), np.array(mask1), np.array(mask1), np.array(mask1),\n",
        "                                  np.array(mask2), np.array(mask2), np.array(mask2), np.array(mask2)]\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indices after each epoch'\n",
        "        self.indices = np.arange(len(self.images))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "    def data_generation(self, indices):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((len(indices), self.img_size, self.img_size, self.n_channels))\n",
        "        y1 = np.empty((len(indices), self.img_size, self.img_size, 1))\n",
        "        y2 = np.empty((len(indices), self.img_size, self.img_size, 3)) # 3 classes (Nuclei, Border, Background)\n",
        "        # Generate data\n",
        "        for n, i in enumerate(indices):\n",
        "            X[n] = self.images[i]\n",
        "            y1[n] = (self.masks[i]/255.)[..., np.newaxis]\n",
        "            y2[n] = self.masks_cat[i]\n",
        "\n",
        "        return np.uint8(X), [np.float32(y1), np.float32(y2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNrD-xJTRYqj"
      },
      "outputs": [],
      "source": [
        "AUGMENTATIONS_TRAIN = A.Compose([\n",
        "    A.Rotate(limit=360, p=0.5),\n",
        "    A.OneOf([\n",
        "        A.HorizontalFlip(),\n",
        "        A.VerticalFlip(),\n",
        "        ], p=0.5),\n",
        "    A.OneOf([\n",
        "        A.RandomBrightnessContrast(),\n",
        "        A.RandomGamma(),\n",
        "        A.GaussNoise()\n",
        "         ], p=0.3),\n",
        "    A.OneOf([\n",
        "        A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
        "        # A.ElasticTransform(alpha=3, sigma=150, alpha_affine=150),\n",
        "        # A.ElasticTransform(),\n",
        "        A.Affine(translate_percent=0.2, shear=30, mode=cv2.BORDER_CONSTANT),\n",
        "        A.GridDistortion(),\n",
        "        A.OpticalDistortion(distort_limit=2, shift_limit=0.5),\n",
        "        ], p=0.3),\n",
        "    A.OneOf([\n",
        "        A.RGBShift(r_shift_limit=40, g_shift_limit=40,  b_shift_limit=40),\n",
        "        A.ColorJitter(hue=0.1),\n",
        "        A.Blur(blur_limit=3)\n",
        "        ], p=0.3),\n",
        "    A.ToFloat(max_value=255)\n",
        "], p=1,\n",
        "additional_targets={'image': 'image', 'mask1': 'mask', 'mask2':'mask'})\n",
        "\n",
        "AUGMENTATIONS_VAL = A.Compose([\n",
        "    A.ToFloat(max_value=255)\n",
        "], p=1,\n",
        "additional_targets={'image': 'image', 'mask1': 'mask', 'mask2':'mask'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sahhqd3vRjCP"
      },
      "source": [
        "### Testing data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8-9L49JRf7C"
      },
      "outputs": [],
      "source": [
        "# Single tissue image with 256*256 tiles (50% overlap between tiles) without augmentation\n",
        "a = DataGenerator(train_images, train_masks, train_masks_cat, batch_size=49, augmentations=AUGMENTATIONS_TRAIN, shuffle=False)\n",
        "images, masks = a.__getitem__(0)\n",
        "\n",
        "max_images = 49\n",
        "grid_width = 7\n",
        "grid_height = int(max_images / grid_width)\n",
        "fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*2, grid_height*2))\n",
        "\n",
        "for i,(im, mask1, mask2) in enumerate(zip(images, masks[0], masks[4])):\n",
        "    ax = axs[int(i / grid_width), i % grid_width]\n",
        "    ax.imshow(im)\n",
        "    # ax.imshow(mask1.squeeze(), alpha=0.4)\n",
        "    ax.imshow(mask2, alpha=0.4)\n",
        "    ax.axis('off')\n",
        "\n",
        "print(mask1.shape)\n",
        "print(mask2.shape)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjqpdPNXTZ66"
      },
      "source": [
        "## Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOCU6VMLkSIy"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# import keras\n",
        "# print(tf.__version__)\n",
        "# print(keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzBhjLo2ayHx"
      },
      "outputs": [],
      "source": [
        "# https://github.com/yingkaisha/keras-unet-collection\n",
        "!pip install keras_unet_collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNCMjZPYa5xB"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"unet_plusplus_2d.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1khOW6qBIJD-HY7LDQxJScICMMDzFbCGG\n",
        "\"\"\"\n",
        "\n",
        "from keras_unet_collection.layer_utils import *\n",
        "from keras_unet_collection.activations import GELU, Snake\n",
        "from keras_unet_collection._backbone_zoo import backbone_zoo, bach_norm_checker\n",
        "from keras_unet_collection._model_unet_2d import UNET_left, UNET_right\n",
        "\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "\n",
        "import warnings\n",
        "\n",
        "def unet_plus_2d_base(input_tensor, filter_num, stack_num_down=2, stack_num_up=2,\n",
        "                      activation='ReLU', batch_norm=False, pool=True, unpool=True, deep_supervision=False, \n",
        "                      backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='xnet'):\n",
        "    '''\n",
        "    The base of U-net++ with an optional ImageNet-trained backbone\n",
        "    \n",
        "    unet_plus_2d_base(input_tensor, filter_num, stack_num_down=2, stack_num_up=2,\n",
        "                      activation='ReLU', batch_norm=False, pool=True, unpool=True, deep_supervision=False, \n",
        "                      backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='xnet')\n",
        "    \n",
        "    ----------\n",
        "    Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N. and Liang, J., 2018. Unet++: A nested u-net architecture \n",
        "    for medical image segmentation. In Deep Learning in Medical Image Analysis and Multimodal Learning \n",
        "    for Clinical Decision Support (pp. 3-11). Springer, Cham.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        input_tensor: the input tensor of the base, e.g., `keras.layers.Inpyt((None, None, 3))`.\n",
        "        filter_num: a list that defines the number of filters for each \\\n",
        "                    down- and upsampling levels. e.g., `[64, 128, 256, 512]`.\n",
        "                    The depth is expected as `len(filter_num)`.\n",
        "        stack_num_down: number of convolutional layers per downsampling level/block. \n",
        "        stack_num_up: number of convolutional layers (after concatenation) per upsampling level/block.\n",
        "        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interfaces, e.g., 'ReLU'.\n",
        "        batch_norm: True for batch normalization.\n",
        "        pool: True or 'max' for MaxPooling2D.\n",
        "              'ave' for AveragePooling2D.\n",
        "              False for strided conv + batch norm + activation.\n",
        "        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n",
        "                'nearest' for Upsampling2D with nearest interpolation.\n",
        "                False for Conv2DTranspose + batch norm + activation.   \n",
        "        deep_supervision: True for a model that supports deep supervision. Details see Zhou et al. (2018).\n",
        "        name: prefix of the created keras model and its layers.\n",
        "        \n",
        "        ---------- (keywords of backbone options) ----------\n",
        "        backbone_name: the bakcbone model name. Should be one of the `tensorflow.keras.applications` class.\n",
        "                       None (default) means no backbone. \n",
        "                       Currently supported backbones are:\n",
        "                       (1) VGG16, VGG19\n",
        "                       (2) ResNet50, ResNet101, ResNet152\n",
        "                       (3) ResNet50V2, ResNet101V2, ResNet152V2\n",
        "                       (4) DenseNet121, DenseNet169, DenseNet201\n",
        "                       (5) EfficientNetB[0-7]\n",
        "        weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), \n",
        "                 or the path to the weights file to be loaded.\n",
        "        freeze_backbone: True for a frozen backbone.\n",
        "        freeze_batch_norm: False for not freezing batch normalization layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        If deep_supervision = False; Then the output is a tensor.\n",
        "        If deep_supervision = True; Then the output is a list of tensors\n",
        "            with the first tensor obtained from the first downsampling level (for checking the input/output shapes only),\n",
        "            the second to the `depth-1`-th tensors obtained from each intermediate upsampling levels (deep supervision tensors),\n",
        "            and the last tensor obtained from the end of the base.\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    activation_func = eval(activation)\n",
        "\n",
        "    depth_ = len(filter_num)\n",
        "    # allocate nested lists for collecting output tensors \n",
        "    X_nest_skip_1 = [[] for _ in range(depth_)]\n",
        "    X_nest_skip_2 = [[] for _ in range(depth_)]\n",
        "\n",
        "    # no backbone cases\n",
        "    if backbone is None:\n",
        "\n",
        "        X = input_tensor\n",
        "\n",
        "        # downsampling blocks (same as in 'unet_2d')\n",
        "        X = CONV_stack(X, filter_num[0], stack_num=stack_num_down, activation=activation, \n",
        "                       batch_norm=batch_norm, name='{}_down0'.format(name))\n",
        "        X_nest_skip_1[0].append(X)\n",
        "        X_nest_skip_2[0].append(X)\n",
        "        for i, f in enumerate(filter_num[1:]):\n",
        "            X = UNET_left(X, f, stack_num=stack_num_down, activation=activation, \n",
        "                          pool=pool, batch_norm=batch_norm, name='{}_down{}'.format(name, i+1))        \n",
        "            X_nest_skip_1[0].append(X)\n",
        "            X_nest_skip_2[0].append(X)\n",
        "\n",
        "    # backbone cases\n",
        "    else:        \n",
        "        # handling VGG16 and VGG19 separately\n",
        "        if 'VGG' in backbone:\n",
        "            backbone_ = backbone_zoo(backbone, weights, input_tensor, depth_, freeze_backbone, freeze_batch_norm)\n",
        "            # collecting backbone feature maps\n",
        "            X_nest_skip_1[0] += backbone_([input_tensor,])\n",
        "            X_nest_skip_2[0] += backbone_([input_tensor,])\n",
        "            depth_encode = len(X_nest_skip_1[0])\n",
        "            depth_encode = len(X_nest_skip_2[0])\n",
        "\n",
        "        # for other backbones\n",
        "        else:\n",
        "            backbone_ = backbone_zoo(backbone, weights, input_tensor, depth_-1, freeze_backbone, freeze_batch_norm)\n",
        "            # collecting backbone feature maps\n",
        "            X_nest_skip_1[0] += backbone_([input_tensor,])\n",
        "            X_nest_skip_2[0] += backbone_([input_tensor,])\n",
        "            depth_encode = len(X_nest_skip_1[0]) + 1\n",
        "            depth_encode = len(X_nest_skip_2[0]) + 1\n",
        "\n",
        "        # extra conv2d blocks are applied\n",
        "        # if downsampling levels of a backbone < user-specified downsampling levels\n",
        "        if depth_encode < depth_:\n",
        "\n",
        "            # begins at the deepest available tensor  \n",
        "            X = X_nest_skip_1[0][-1]\n",
        "            X = X_nest_skip_2[0][-1]\n",
        "\n",
        "            # extra downsamplings\n",
        "            for i in range(depth_-depth_encode):\n",
        "                i_real = i + depth_encode\n",
        "\n",
        "                X = UNET_left(X, filter_num[i_real], stack_num=stack_num_down, activation=activation, pool=pool, \n",
        "                              batch_norm=batch_norm, name='{}_down{}'.format(name, i_real+1))\n",
        "                X_nest_skip_1[0].append(X)\n",
        "                X_nest_skip_2[0].append(X)\n",
        "\n",
        "    for nest_lev in range(1, depth_):\n",
        "\n",
        "        # depth difference between the deepest nest skip and the current upsampling  \n",
        "        depth_lev = depth_-nest_lev\n",
        "\n",
        "        # number of available encoded tensors\n",
        "        depth_decode = len(X_nest_skip_1[nest_lev-1])\n",
        "\n",
        "        # loop over individual upsamling levels\n",
        "        for i in range(1, depth_decode):\n",
        "\n",
        "            # collecting previous downsampling outputs\n",
        "            previous_skip = []\n",
        "            for previous_lev in range(nest_lev):\n",
        "                previous_skip.append(X_nest_skip_1[previous_lev][i-1])\n",
        "\n",
        "            # upsamping block that concatenates all available (same feature map size) down-/upsampling outputs\n",
        "            X_nest_skip_1[nest_lev].append(\n",
        "                UNET_right(X_nest_skip_1[nest_lev-1][i], previous_skip, filter_num[i-1], \n",
        "                           stack_num=stack_num_up, activation=activation, unpool=unpool, \n",
        "                           batch_norm=batch_norm, concat=True, name='{}_up{}_1_from{}'.format(name, nest_lev-1, i-1)))\n",
        "\n",
        "        if depth_decode < depth_lev+1:\n",
        "\n",
        "            X = X_nest_skip_1[nest_lev-1][-1]\n",
        "\n",
        "            for j in range(depth_lev-depth_decode+1):\n",
        "                j_real = j + depth_decode\n",
        "                X = UNET_right(X, None, filter_num[j_real-1], \n",
        "                               stack_num=stack_num_up, activation=activation, unpool=unpool, \n",
        "                               batch_norm=batch_norm, concat=True, name='{}_up{}_from{}'.format(name, nest_lev-1, j_real-1))\n",
        "                X_nest_skip_1[nest_lev].append(X)\n",
        "\n",
        "    for nest_lev in range(1, depth_):\n",
        "\n",
        "        # depth difference between the deepest nest skip and the current upsampling  \n",
        "        depth_lev = depth_-nest_lev\n",
        "\n",
        "        # number of available encoded tensors\n",
        "        depth_decode = len(X_nest_skip_2[nest_lev-1])\n",
        "\n",
        "        # loop over individual upsamling levels\n",
        "        for i in range(1, depth_decode):\n",
        "\n",
        "            # collecting previous downsampling outputs\n",
        "            previous_skip = []\n",
        "            for previous_lev in range(nest_lev):\n",
        "                previous_skip.append(X_nest_skip_2[previous_lev][i-1])\n",
        "\n",
        "            # upsamping block that concatenates all available (same feature map size) down-/upsampling outputs\n",
        "            X_nest_skip_2[nest_lev].append(\n",
        "                UNET_right(X_nest_skip_2[nest_lev-1][i], previous_skip, filter_num[i-1], \n",
        "                           stack_num=stack_num_up, activation=activation, unpool=unpool, \n",
        "                           batch_norm=batch_norm, concat=True, name='{}_up{}_2_from{}'.format(name, nest_lev-1, i-1)))\n",
        "\n",
        "        if depth_decode < depth_lev+1:\n",
        "\n",
        "            X = X_nest_skip_2[nest_lev-1][-1]\n",
        "\n",
        "            for j in range(depth_lev-depth_decode+1):\n",
        "                j_real = j + depth_decode\n",
        "                X = UNET_right(X, None, filter_num[j_real-1], \n",
        "                               stack_num=stack_num_up, activation=activation, unpool=unpool, \n",
        "                               batch_norm=batch_norm, concat=True, name='{}_up{}_2_from{}'.format(name, nest_lev-1, j_real-1))\n",
        "                X_nest_skip_2[nest_lev].append(X)\n",
        "            \n",
        "    # output\n",
        "    if deep_supervision:\n",
        "        \n",
        "        X_list_1 = []\n",
        "        X_list_2 = []\n",
        "        \n",
        "        for i in range(depth_):\n",
        "            X_list_1.append(X_nest_skip_1[i][0])\n",
        "            X_list_2.append(X_nest_skip_2[i][0])\n",
        "        \n",
        "        return X_list_1, X_list_2\n",
        "        \n",
        "    else:\n",
        "        return X_nest_skip_1[-1][0], X_nest_skip_2[-1][0]\n",
        "\n",
        "def unet_plus_2d(input_size, filter_num, n_labels, stack_num_down=2, stack_num_up=2,\n",
        "                 activation='ReLU', output_activation='Softmax', batch_norm=False, pool=True, unpool=True, deep_supervision=False, \n",
        "                 backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='xnet'):\n",
        "    '''\n",
        "    U-net++ with an optional ImageNet-trained backbone.\n",
        "    \n",
        "    unet_plus_2d(input_size, filter_num, n_labels, stack_num_down=2, stack_num_up=2,\n",
        "                 activation='ReLU', output_activation='Softmax', batch_norm=False, pool=True, unpool=True, deep_supervision=False, \n",
        "                 backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='xnet')\n",
        "    \n",
        "    ----------\n",
        "    Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N. and Liang, J., 2018. Unet++: A nested u-net architecture \n",
        "    for medical image segmentation. In Deep Learning in Medical Image Analysis and Multimodal Learning \n",
        "    for Clinical Decision Support (pp. 3-11). Springer, Cham.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        input_size: the size/shape of network input, e.g., `(128, 128, 3)`.\n",
        "        filter_num: a list that defines the number of filters for each \\\n",
        "                    down- and upsampling levels. e.g., `[64, 128, 256, 512]`.\n",
        "                    The depth is expected as `len(filter_num)`.\n",
        "        n_labels: number of output labels.\n",
        "        stack_num_down: number of convolutional layers per downsampling level/block. \n",
        "        stack_num_up: number of convolutional layers (after concatenation) per upsampling level/block.\n",
        "        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interfaces, e.g., 'ReLU'.\n",
        "        output_activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interface or 'Sigmoid'.\n",
        "                           Default option is 'Softmax'.\n",
        "                           if None is received, then linear activation is applied.\n",
        "        batch_norm: True for batch normalization.\n",
        "        pool: True or 'max' for MaxPooling2D.\n",
        "              'ave' for AveragePooling2D.\n",
        "              False for strided conv + batch norm + activation.\n",
        "        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n",
        "                'nearest' for Upsampling2D with nearest interpolation.\n",
        "                False for Conv2DTranspose + batch norm + activation.   \n",
        "        deep_supervision: True for a model that supports deep supervision. Details see Zhou et al. (2018).\n",
        "        name: prefix of the created keras model and its layers.\n",
        "        \n",
        "        ---------- (keywords of backbone options) ----------\n",
        "        backbone_name: the bakcbone model name. Should be one of the `tensorflow.keras.applications` class.\n",
        "                       None (default) means no backbone. \n",
        "                       Currently supported backbones are:\n",
        "                       (1) VGG16, VGG19\n",
        "                       (2) ResNet50, ResNet101, ResNet152\n",
        "                       (3) ResNet50V2, ResNet101V2, ResNet152V2\n",
        "                       (4) DenseNet121, DenseNet169, DenseNet201\n",
        "                       (5) EfficientNetB[0-7]\n",
        "        weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), \n",
        "                 or the path to the weights file to be loaded.\n",
        "        freeze_backbone: True for a frozen backbone.\n",
        "        freeze_batch_norm: False for not freezing batch normalization layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        model: a keras model.\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    depth_ = len(filter_num)\n",
        "    \n",
        "    if backbone is not None:\n",
        "        bach_norm_checker(backbone, batch_norm)\n",
        "    \n",
        "    IN = Input(input_size)\n",
        "    # base\n",
        "    X_1, X_2 = unet_plus_2d_base(IN, filter_num, stack_num_down=stack_num_down, stack_num_up=stack_num_up,\n",
        "                          activation=activation, batch_norm=batch_norm, pool=pool, unpool=unpool, deep_supervision=deep_supervision, \n",
        "                          backbone=backbone, weights=weights, freeze_backbone=freeze_backbone, freeze_batch_norm=freeze_batch_norm, name=name)\n",
        "    \n",
        "    # output\n",
        "    if deep_supervision:\n",
        "        \n",
        "        if (backbone is not None) and freeze_backbone:\n",
        "            backbone_warn = '\\n\\nThe shallowest U-net++ deep supervision branch directly connects to a frozen backbone.\\nTesting your configurations on `keras_unet_collection.base.unet_plus_2d_base` is recommended.'\n",
        "            warnings.warn(backbone_warn);\n",
        "            \n",
        "        # model base returns a list of tensors\n",
        "        X_list_1, X_list_2 = X_1, X_2\n",
        "        OUT_list = []\n",
        "\n",
        "        \n",
        "        print('----------\\ndeep_supervision = True\\nnames of output tensors are listed as follows (\"sup0\" is the shallowest supervision layer;\\n\"final\" is the final output layer):\\n')\n",
        "        \n",
        "        # no backbone or VGG backbones\n",
        "        # depth_ > 2 is expected (a least two downsampling blocks)\n",
        "        if (backbone is None) or 'VGG' in backbone:\n",
        "        \n",
        "            for i in range(1, depth_-1):\n",
        "                if output_activation is None:\n",
        "                    print('\\t{}_1_output_sup{}'.format(name, i))\n",
        "                else:\n",
        "                    print('\\t{}_1_output_sup{}_activation'.format(name, i))\n",
        "                    \n",
        "                OUT_list.append(CONV_output(X_list_1[i], 1, kernel_size=1, activation=\"Sigmoid\", \n",
        "                                            name='{}_1_output_sup{}'.format(name, i)))\n",
        "        # other backbones        \n",
        "        else:\n",
        "            for i in range(1, depth_-1):\n",
        "                if output_activation is None:\n",
        "                    print('\\t{}_output_sup{}'.format(name, i-1))\n",
        "                else:\n",
        "                    print('\\t{}_output_sup{}_activation'.format(name, i-1))\n",
        "                \n",
        "                # an extra upsampling for creating full resolution feature maps\n",
        "                X = decode_layer(X_list_1[i], filter_num[i], 2, unpool, activation=activation, \n",
        "                                 batch_norm=batch_norm, name='{}_sup{}_up'.format(name, i-1))\n",
        "                \n",
        "                X = CONV_output(X, n_labels, kernel_size=1, activation=output_activation, name='{}_output_sup{}'.format(name, i-1))\n",
        "                OUT_list.append(X)\n",
        "                \n",
        "        if output_activation is None:\n",
        "            print('\\t{}_output_final'.format(name))\n",
        "        else:\n",
        "            print('\\t{}_1_output_final_activation'.format(name))\n",
        "            \n",
        "        OUT_list.append(CONV_output(X_list_1[-1], 1, kernel_size=1, activation=\"Sigmoid\", name='{}_1_output_final'.format(name)))\n",
        "\n",
        "        # no backbone or VGG backbones\n",
        "        # depth_ > 2 is expected (a least two downsampling blocks)\n",
        "        if (backbone is None) or 'VGG' in backbone:\n",
        "        \n",
        "            for i in range(1, depth_-1):\n",
        "                if output_activation is None:\n",
        "                    print('\\t{}_2_output_sup{}'.format(name, i))\n",
        "                else:\n",
        "                    print('\\t{}_2_output_sup{}_activation'.format(name, i))\n",
        "                    \n",
        "                OUT_list.append(CONV_output(X_list_2[i], 3, kernel_size=1, activation=\"Softmax\", \n",
        "                                            name='{}_2_output_sup{}'.format(name, i)))\n",
        "        # other backbones        \n",
        "        else:\n",
        "            for i in range(1, depth_-1):\n",
        "                if output_activation is None:\n",
        "                    print('\\t{}_output_sup{}'.format(name, i-1))\n",
        "                else:\n",
        "                    print('\\t{}_output_sup{}_activation'.format(name, i-1))\n",
        "                \n",
        "                # an extra upsampling for creating full resolution feature maps\n",
        "                X = decode_layer(X_list_2[i], filter_num[i], 2, unpool, activation=activation, \n",
        "                                 batch_norm=batch_norm, name='{}_sup{}_up'.format(name, i-1))\n",
        "                \n",
        "                X = CONV_output(X, n_labels, kernel_size=1, activation=output_activation, name='{}_output_sup{}'.format(name, i-1))\n",
        "                OUT_list.append(X)\n",
        "                \n",
        "        if output_activation is None:\n",
        "            print('\\t{}_output_final'.format(name))\n",
        "        else:\n",
        "            print('\\t{}_2_output_final_activation'.format(name))\n",
        "            \n",
        "        OUT_list.append(CONV_output(X_list_2[-1], 3, kernel_size=1, activation=\"Softmax\", name='{}_2_output_final'.format(name)))\n",
        "        \n",
        "    else:\n",
        "        OUT_1 = CONV_output(X_1, 1, kernel_size=1, activation='Sigmoid', name='Binary_segmentation')\n",
        "        OUT_2 = CONV_output(X_2, 3, kernel_size=1, activation='Softmax', name='MultiClass_segmentation')\n",
        "        OUT_list = [OUT_1, OUT_2]\n",
        "        \n",
        "    # model\n",
        "    model = Model(inputs=[IN,], outputs=OUT_list, name='{}_model'.format(name))\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTHG5ruqLD_m"
      },
      "outputs": [],
      "source": [
        "model = unet_plus_2d(input_size=(256, 256, 3),\n",
        "                     filter_num=[16, 32, 64, 128, 256],\n",
        "                     n_labels=1,\n",
        "                     batch_norm=True,\n",
        "                     deep_supervision=True,\n",
        "                     backbone=\"VGG19\",\n",
        "                     freeze_backbone=False,\n",
        "                     freeze_batch_norm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5McDn_dWU6I"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jC-eAphWUub"
      },
      "outputs": [],
      "source": [
        "from keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpGYX_INWXgp"
      },
      "outputs": [],
      "source": [
        "plot_model(model, show_shapes=True, dpi=330, to_file=\"U-Net++_DD.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ8oAgALaj8-"
      },
      "source": [
        "### Defining loss function for binary & multi segmentation tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as9BTX7-Zpg0"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efXpJJ4QfjXK"
      },
      "outputs": [],
      "source": [
        "from keras_unet_collection import losses\n",
        "\n",
        "def binary_loss(y_true, y_pred):\n",
        "\n",
        "    loss_focal = losses.focal_tversky(y_true, y_pred, alpha=0.5, gamma=4/3)\n",
        "    loss_iou = losses.iou_seg(y_true, y_pred)\n",
        "    \n",
        "    # (x) \n",
        "    # loss_ssim = losses.ms_ssim(y_true, y_pred, max_val=1.0, filter_size=4)\n",
        "    \n",
        "    return loss_focal+loss_iou #+loss_ssim\n",
        "\n",
        "def multi_loss(y_true, y_pred):\n",
        "\n",
        "    loss_focal = losses.focal_tversky(y_true, y_pred, alpha=0.7, gamma=4/3)\n",
        "    loss_iou = losses.iou_seg(y_true, y_pred)\n",
        "    \n",
        "    # (x) \n",
        "    # loss_ssim = losses.ms_ssim(y_true, y_pred, max_val=1.0, filter_size=4)\n",
        "    \n",
        "    return loss_focal+loss_iou #+loss_ssim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUN_1pCkaDZW"
      },
      "outputs": [],
      "source": [
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ET3WNbwb3U-"
      },
      "source": [
        "### Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHRIkAr_bWjn"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=Adam(1e-4),\n",
        "              loss={\n",
        "                     \"xnet_1_output_sup1_activation\":binary_loss,\n",
        "\t                 \"xnet_1_output_sup2_activation\":binary_loss,\n",
        "\t                 \"xnet_1_output_sup3_activation\":binary_loss,\n",
        "\t                 \"xnet_1_output_final_activation\":binary_loss,\n",
        "\t                 \"xnet_2_output_sup1_activation\":multi_loss,\n",
        "\t                 \"xnet_2_output_sup2_activation\":multi_loss,\n",
        "\t                 \"xnet_2_output_sup3_activation\":multi_loss,\n",
        "\t                 \"xnet_2_output_final_activation\":multi_loss\n",
        "                     },\n",
        "              loss_weights=[0.25, 0.35, 0.45, 1.0, 0.25, 0.35, 0.45, 1.0],\n",
        "              metrics={\n",
        "                     \"xnet_1_output_sup1_activation\":dice_coef,\n",
        "\t                 \"xnet_1_output_sup2_activation\":dice_coef,\n",
        "\t                 \"xnet_1_output_sup3_activation\":dice_coef,\n",
        "\t                 \"xnet_1_output_final_activation\":dice_coef,\n",
        "\t                 \"xnet_2_output_sup1_activation\":dice_coef,\n",
        "\t                 \"xnet_2_output_sup2_activation\":dice_coef,\n",
        "\t                 \"xnet_2_output_sup3_activation\":dice_coef,\n",
        "\t                 \"xnet_2_output_final_activation\":dice_coef\n",
        "                       })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NY7qhfHdjT9"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwcWk3exb1Dt"
      },
      "source": [
        "### Defining some useful callbacks "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADk25mhZcCsi"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, Callback, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
        "import datetime\n",
        "\n",
        "from skimage import filters\n",
        "from scipy.ndimage import measurements\n",
        "from skimage.segmentation import watershed, mark_boundaries\n",
        "\n",
        "# Visualize training \n",
        "class loss_history(Callback):\n",
        "\n",
        "    def __init__(self, x=4):\n",
        "        self.x = x\n",
        "    \n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        fig, ax = plt.subplots(1, 5, figsize=(18, 12))\n",
        "        [axi.set_axis_off() for axi in ax.ravel()]\n",
        "\n",
        "        ax[0].imshow(train_images[self.x])\n",
        "        ax[0].set_title(\"Tissue Image\")\n",
        "\n",
        "        ax[1].imshow(train_masks[self.x], cmap=\"gray\")\n",
        "        ax[1].set_title(\"Ground Truth\")\n",
        "\n",
        "        model_sample_input = train_images[self.x].astype(\"float32\") / 255.\n",
        "        pred = self.model.predict(np.expand_dims(model_sample_input, axis=0), verbose=0)\n",
        "        preds_train1 = (pred[0] + pred[1] + pred[2] + pred[3]) / 4\n",
        "        preds_train2 = (pred[4] + pred[5] + pred[6] + pred[7]) / 4\n",
        "\n",
        "        preds1 =  np.squeeze(preds_train1[0]) >= 0.5\n",
        "        ax[2].imshow(preds1, cmap=\"gray\")\n",
        "        ax[2].set_title(\"Nuclei prediction\")\n",
        "        preds2 = preds_train2[0][:, :, 2] - preds_train2[0][:, :, 1] >= 0.5\n",
        "        ax[3].imshow(preds2, cmap=\"gray\")\n",
        "        ax[3].set_title(\"Nuclei marker prediction\")\n",
        "        \n",
        "        grad = filters.scharr(preds1)\n",
        "        marker = preds1 * preds2\n",
        "        marker = measurements.label(marker)[0]\n",
        "        proced_pred = watershed(grad, marker, mask=preds1)\n",
        "        ax[4].imshow(mark_boundaries(train_images[self.x], proced_pred, color=(0, 0, 1)))\n",
        "        ax[4].set_title(\"Result\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# lr_scheduler = CosineDecay(\n",
        "#     1e-4, 50, alpha=0.0, name=None\n",
        "# )\n",
        "\n",
        "create_path(\"logs/Unet++\")\n",
        "csv_log = CSVLogger('logs/Unet++/Unet++_fold1_log00.csv', separator=',')\n",
        "model_name = f\"Unet++_fold1_v00_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        "create_path(\"models/Unet++\")\n",
        "path_to_save_model = \"models/Unet++/\" + model_name + \".h5\"\n",
        "checkpointer = ModelCheckpoint(path_to_save_model, verbose=1, save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-7, verbose=1)\n",
        "# reduce_lr = LearningRateScheduler(schedule=lr_scheduler)\n",
        "early_stop   = EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=False)\n",
        "\n",
        "print(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWlQhnxEdBDc"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "             loss_history(), \n",
        "             checkpointer, \n",
        "             reduce_lr, \n",
        "            #  csv_log, \n",
        "             early_stop\n",
        "             ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7X3dJTMdn5t"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "epochs = 200\n",
        "\n",
        "# Generators\n",
        "training_generator = DataGenerator(train_images, train_masks, train_masks_cat, augmentations=AUGMENTATIONS_TRAIN, batch_size=batch_size)\n",
        "validation_generator = DataGenerator(val_images, val_masks, val_masks_cat, augmentations=AUGMENTATIONS_VAL, batch_size=batch_size)\n",
        "\n",
        "history = model.fit(training_generator,\n",
        "                    validation_data=validation_generator,                        \n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_OYcrtwmjcE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}