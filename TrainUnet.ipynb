{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFlcLu2vJ3Mc"
      },
      "outputs": [],
      "source": [
        "# https://github.com/qubvel/segmentation_models\n",
        "!pip install segmentation-models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gauMLtbtKHJ-"
      },
      "outputs": [],
      "source": [
        "\"\"\"replace this with segmentation_models.models.unet.py and restart runtime\"\"\"\n",
        "# from keras_applications import get_submodules_from_kwargs\n",
        "\n",
        "# from ._common_blocks import Conv2dBn\n",
        "# from ._utils import freeze_model\n",
        "# from ..backbones.backbones_factory import Backbones\n",
        "\n",
        "# backend = None\n",
        "# layers = None\n",
        "# models = None\n",
        "# keras_utils = None\n",
        "\n",
        "\n",
        "# # ---------------------------------------------------------------------\n",
        "# #  Utility functions\n",
        "# # ---------------------------------------------------------------------\n",
        "\n",
        "# def get_submodules():\n",
        "#     return {\n",
        "#         'backend': backend,\n",
        "#         'models': models,\n",
        "#         'layers': layers,\n",
        "#         'utils': keras_utils,\n",
        "#     }\n",
        "\n",
        "\n",
        "# # ---------------------------------------------------------------------\n",
        "# #  Blocks\n",
        "# # ---------------------------------------------------------------------\n",
        "\n",
        "# def Conv3x3BnReLU(filters, use_batchnorm, name=None):\n",
        "#     kwargs = get_submodules()\n",
        "\n",
        "#     def wrapper(input_tensor):\n",
        "#         return Conv2dBn(\n",
        "#             filters,\n",
        "#             kernel_size=3,\n",
        "#             activation='relu',\n",
        "#             kernel_initializer='he_uniform',\n",
        "#             padding='same',\n",
        "#             use_batchnorm=use_batchnorm,\n",
        "#             name=name,\n",
        "#             **kwargs\n",
        "#         )(input_tensor)\n",
        "\n",
        "#     return wrapper\n",
        "\n",
        "\n",
        "# def DecoderUpsamplingX2Block(filters, decoder_num, stage, use_batchnorm=False):\n",
        "#     up_name = 'decoder{}_stage{}_upsampling'.format(decoder_num, stage)\n",
        "#     conv1_name = 'decoder{}_stage{}a'.format(decoder_num, stage)\n",
        "#     conv2_name = 'decoder{}_stage{}b'.format(decoder_num, stage)\n",
        "#     concat_name = 'decoder{}_stage{}_concat'.format(decoder_num, stage)\n",
        "\n",
        "#     concat_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
        "\n",
        "#     def wrapper(input_tensor, skip=None):\n",
        "#         x = layers.UpSampling2D(size=2, name=up_name)(input_tensor)\n",
        "\n",
        "#         if skip is not None:\n",
        "#             x = layers.Concatenate(axis=concat_axis, name=concat_name)([x, skip])\n",
        "\n",
        "#         x = Conv3x3BnReLU(filters, use_batchnorm, name=conv1_name)(x)\n",
        "#         x = Conv3x3BnReLU(filters, use_batchnorm, name=conv2_name)(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "#     return wrapper\n",
        "\n",
        "\n",
        "# def DecoderTransposeX2Block(filters, decoder_num, stage, use_batchnorm=False):\n",
        "#     transp_name = 'decoder{}_stage{}a_transpose'.format(decoder_num, stage)\n",
        "#     bn_name = 'decoder{}_stage{}a_bn'.format(decoder_num, stage)\n",
        "#     relu_name = 'decoder{}_stage{}a_relu'.format(decoder_num, stage)\n",
        "#     conv_block_name = 'decoder{}_stage{}b'.format(decoder_num, stage)\n",
        "#     concat_name = 'decoder{}_stage{}_concat'.format(decoder_num, stage)\n",
        "\n",
        "#     concat_axis = bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
        "\n",
        "#     def layer(input_tensor, skip=None):\n",
        "\n",
        "#         x = layers.Conv2DTranspose(\n",
        "#             filters,\n",
        "#             kernel_size=(4, 4),\n",
        "#             strides=(2, 2),\n",
        "#             padding='same',\n",
        "#             name=transp_name,\n",
        "#             use_bias=not use_batchnorm,\n",
        "#         )(input_tensor)\n",
        "\n",
        "#         if use_batchnorm:\n",
        "#             x = layers.BatchNormalization(axis=bn_axis, name=bn_name)(x)\n",
        "\n",
        "#         x = layers.Activation('relu', name=relu_name)(x)\n",
        "\n",
        "#         if skip is not None:\n",
        "#             x = layers.Concatenate(axis=concat_axis, name=concat_name)([x, skip])\n",
        "\n",
        "#         x = Conv3x3BnReLU(filters, use_batchnorm, name=conv_block_name)(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "#     return layer\n",
        "\n",
        "\n",
        "# # ---------------------------------------------------------------------\n",
        "# #  Unet Decoder\n",
        "# # ---------------------------------------------------------------------\n",
        "\n",
        "# def build_unet(\n",
        "#         backbone,\n",
        "#         decoder_block,\n",
        "#         skip_connection_layers,\n",
        "#         decoder_filters=(256, 128, 64, 32, 16),\n",
        "#         n_upsample_blocks=5,\n",
        "#         classes=1,\n",
        "#         activation='sigmoid',\n",
        "#         use_batchnorm=True,\n",
        "# ):\n",
        "#     input_ = backbone.input\n",
        "#     x1 = backbone.output\n",
        "#     x2 = backbone.output\n",
        "\n",
        "#     # extract skip connections\n",
        "#     skips = ([backbone.get_layer(name=i).output if isinstance(i, str)\n",
        "#               else backbone.get_layer(index=i).output for i in skip_connection_layers])\n",
        "\n",
        "#     # add center block if previous operation was maxpooling (for vgg models)\n",
        "#     if isinstance(backbone.layers[-1], layers.MaxPooling2D):\n",
        "#         x1 = Conv3x3BnReLU(512, use_batchnorm, name='center_block1_1')(x1)\n",
        "#         x1 = Conv3x3BnReLU(512, use_batchnorm, name='center_block2_1')(x1)\n",
        "\n",
        "#     # add center block if previous operation was maxpooling (for vgg models)\n",
        "#     if isinstance(backbone.layers[-1], layers.MaxPooling2D):\n",
        "#         x2 = Conv3x3BnReLU(512, use_batchnorm, name='center_block1_2')(x2)\n",
        "#         x2 = Conv3x3BnReLU(512, use_batchnorm, name='center_block2_2')(x2)\n",
        "\n",
        "#     # building decoder blocks\n",
        "#     for i in range(n_upsample_blocks):\n",
        "\n",
        "#         if i < len(skips):\n",
        "#             skip = skips[i]\n",
        "#         else:\n",
        "#             skip = None\n",
        "\n",
        "#         x1 = decoder_block(decoder_filters[i], decoder_num=1, stage=i, use_batchnorm=use_batchnorm)(x1, skip)\n",
        "\n",
        "#     # building decoder blocks\n",
        "#     for i in range(n_upsample_blocks):\n",
        "\n",
        "#         if i < len(skips):\n",
        "#             skip = skips[i]\n",
        "#         else:\n",
        "#             skip = None\n",
        "\n",
        "#         x2 = decoder_block(decoder_filters[i], decoder_num=2, stage=i, use_batchnorm=use_batchnorm)(x2, skip)\n",
        "\n",
        "\n",
        "#     # model head (define number of output classes)\n",
        "#     x1 = layers.Conv2D(\n",
        "#         filters=1,\n",
        "#         kernel_size=(3, 3),\n",
        "#         padding='same',\n",
        "#         use_bias=True,\n",
        "#         kernel_initializer='glorot_uniform',\n",
        "#         name='final_conv_1',\n",
        "#     )(x1)\n",
        "#     x1 = layers.Activation(\"sigmoid\", name=\"Binary_segmentation\")(x1)\n",
        "\n",
        "#     x2 = layers.Conv2D(\n",
        "#         filters=3,\n",
        "#         kernel_size=(3, 3),\n",
        "#         padding='same',\n",
        "#         use_bias=True,\n",
        "#         kernel_initializer='glorot_uniform',\n",
        "#         name='final_conv_2',\n",
        "#     )(x2)\n",
        "#     x2 = layers.Activation(\"softmax\", name=\"MultiClass_segmentation\")(x2)\n",
        "\n",
        "#     # create keras model instance\n",
        "#     model = models.Model(input_, [x1, x2])\n",
        "\n",
        "#     return model\n",
        "\n",
        "\n",
        "# # ---------------------------------------------------------------------\n",
        "# #  Unet Model\n",
        "# # ---------------------------------------------------------------------\n",
        "\n",
        "# def Unet(\n",
        "#         backbone_name='vgg16',\n",
        "#         input_shape=(None, None, 3),\n",
        "#         classes=1,\n",
        "#         activation='sigmoid',\n",
        "#         weights=None,\n",
        "#         encoder_weights='imagenet',\n",
        "#         encoder_freeze=False,\n",
        "#         encoder_features='default',\n",
        "#         decoder_block_type='upsampling',\n",
        "#         decoder_filters=(256, 128, 64, 32, 16),\n",
        "#         decoder_use_batchnorm=True,\n",
        "#         **kwargs\n",
        "# ):\n",
        "#     \"\"\" Unet_ is a fully convolution neural network for image semantic segmentation\n",
        "\n",
        "#     Args:\n",
        "#         backbone_name: name of classification model (without last dense layers) used as feature\n",
        "#             extractor to build segmentation model.\n",
        "#         input_shape: shape of input data/image ``(H, W, C)``, in general\n",
        "#             case you do not need to set ``H`` and ``W`` shapes, just pass ``(None, None, C)`` to make your model be\n",
        "#             able to process images af any size, but ``H`` and ``W`` of input images should be divisible by factor ``32``.\n",
        "#         classes: a number of classes for output (output shape - ``(h, w, classes)``).\n",
        "#         activation: name of one of ``keras.activations`` for last model layer\n",
        "#             (e.g. ``sigmoid``, ``softmax``, ``linear``).\n",
        "#         weights: optional, path to model weights.\n",
        "#         encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n",
        "#         encoder_freeze: if ``True`` set all layers of encoder (backbone model) as non-trainable.\n",
        "#         encoder_features: a list of layer numbers or names starting from top of the model.\n",
        "#             Each of these layers will be concatenated with corresponding decoder block. If ``default`` is used\n",
        "#             layer names are taken from ``DEFAULT_SKIP_CONNECTIONS``.\n",
        "#         decoder_block_type: one of blocks with following layers structure:\n",
        "\n",
        "#             - `upsampling`:  ``UpSampling2D`` -> ``Conv2D`` -> ``Conv2D``\n",
        "#             - `transpose`:   ``Transpose2D`` -> ``Conv2D``\n",
        "\n",
        "#         decoder_filters: list of numbers of ``Conv2D`` layer filters in decoder blocks\n",
        "#         decoder_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n",
        "#             is used.\n",
        "\n",
        "#     Returns:\n",
        "#         ``keras.models.Model``: **Unet**\n",
        "\n",
        "#     .. _Unet:\n",
        "#         https://arxiv.org/pdf/1505.04597\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "#     global backend, layers, models, keras_utils\n",
        "#     backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
        "\n",
        "#     if decoder_block_type == 'upsampling':\n",
        "#         decoder_block = DecoderUpsamplingX2Block\n",
        "#     elif decoder_block_type == 'transpose':\n",
        "#         decoder_block = DecoderTransposeX2Block\n",
        "#     else:\n",
        "#         raise ValueError('Decoder block type should be in (\"upsampling\", \"transpose\"). '\n",
        "#                          'Got: {}'.format(decoder_block_type))\n",
        "\n",
        "#     backbone = Backbones.get_backbone(\n",
        "#         backbone_name,\n",
        "#         input_shape=input_shape,\n",
        "#         weights=encoder_weights,\n",
        "#         include_top=False,\n",
        "#         **kwargs,\n",
        "#     )\n",
        "\n",
        "#     if encoder_features == 'default':\n",
        "#         encoder_features = Backbones.get_feature_layers(backbone_name, n=4)\n",
        "\n",
        "#     model = build_unet(\n",
        "#         backbone=backbone,\n",
        "#         decoder_block=decoder_block,\n",
        "#         skip_connection_layers=encoder_features,\n",
        "#         decoder_filters=decoder_filters,\n",
        "#         classes=classes,\n",
        "#         activation=activation,\n",
        "#         n_upsample_blocks=len(decoder_filters),\n",
        "#         use_batchnorm=decoder_use_batchnorm,\n",
        "#     )\n",
        "\n",
        "#     # lock encoder weights for fine-tuning\n",
        "#     if encoder_freeze:\n",
        "#         freeze_model(backbone, **kwargs)\n",
        "\n",
        "#     # loading model weights\n",
        "#     if weights is not None:\n",
        "#         model.load_weights(weights)\n",
        "\n",
        "#     return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTCUA1M6KCkm"
      },
      "outputs": [],
      "source": [
        "import segmentation_models as sm\n",
        "sm.set_framework(\"tf.keras\")\n",
        "sm.framework()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qMrCzY5UFSw"
      },
      "source": [
        "# Train U-Net with MoNuSeg dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2lxyy4JgQB_"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkvEVj1DgUwU"
      },
      "outputs": [],
      "source": [
        "!cat /proc/cpuinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNP2A-Dcjc6O"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcpa5mbeK3dk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7guJCJRsK7MW"
      },
      "outputs": [],
      "source": [
        "%cd drive/MyDrive/nuclei_segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRuQXwqXJhHJ"
      },
      "outputs": [],
      "source": [
        "# https://github.com/dovahcrow/patchify.py\n",
        "!pip install patchify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0SwqgoBK0GL"
      },
      "source": [
        "## Make validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LKl6qvnLM4K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMrJsiMDLcpA"
      },
      "outputs": [],
      "source": [
        "def create_path(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWvuYRd2LPhn"
      },
      "outputs": [],
      "source": [
        "train_dir = \"dataset/monuseg/stain_normalized/train\"\n",
        "val_dir = \"dataset/monuseg/stain_normalized/validation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpCzs_rmLdeQ"
      },
      "outputs": [],
      "source": [
        "def make_validation_set(val_dir, train_dir, val_size=6, seed=42, fold=None):\n",
        "\n",
        "    create_path(val_dir)\n",
        "    create_path(os.path.join(val_dir, \"tissue_images\"))\n",
        "    create_path(os.path.join(val_dir, \"binary_masks\"))\n",
        "    create_path(os.path.join(val_dir, \"instance_masks\"))\n",
        "    create_path(os.path.join(val_dir, \"modified_masks\"))\n",
        "    \n",
        "    for j in sorted(glob.glob(os.path.join(val_dir, \"tissue_images\", \"*\"))):\n",
        "        try:\n",
        "            shutil.move(j, os.path.join(train_dir, \"tissue_images\"))\n",
        "            shutil.move(j.replace(\"tissue_images\", \"binary_masks\").replace(\"tif\", \"png\"), \n",
        "                        os.path.join(train_dir, \"binary_masks\"))\n",
        "            shutil.move(j.replace(\"tissue_images\", \"instance_masks\").replace(\"tif\", \"npy\"), \n",
        "                        os.path.join(train_dir, \"instance_masks\"))\n",
        "            shutil.move(j.replace(\"tissue_images\", \"modified_masks\").replace(\"tif\", \"png\"), \n",
        "                        os.path.join(train_dir, \"modified_masks\"))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    images_lst = sorted(glob.glob(os.path.join(train_dir, \"tissue_images\", \"*\")))\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(images_lst)\n",
        "    if fold is None:\n",
        "        random.seed(seed)\n",
        "        val_lst = random.sample(images_lst, val_size)\n",
        "    else:\n",
        "        val_lst = images_lst[(fold*val_size)-val_size: fold*val_size]\n",
        "        \n",
        "\n",
        "    for i in val_lst:\n",
        "        shutil.move(i, os.path.join(val_dir, \"tissue_images\"))\n",
        "        shutil.move(i.replace(\"tissue_images\", \"binary_masks\").replace(\"tif\", \"png\"), \n",
        "                    os.path.join(val_dir, \"binary_masks\"))\n",
        "        shutil.move(i.replace(\"tissue_images\", \"instance_masks\").replace(\"tif\", \"npy\"), \n",
        "                    os.path.join(val_dir, \"instance_masks\"))\n",
        "        shutil.move(i.replace(\"tissue_images\", \"modified_masks\").replace(\"tif\", \"png\"), \n",
        "                    os.path.join(val_dir, \"modified_masks\"))\n",
        "        \n",
        "    print(f\"Validation list: {[os.path.basename(i) for i in val_lst]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e_pPZeWOXFA"
      },
      "outputs": [],
      "source": [
        "make_validation_set(val_dir, train_dir, val_size=6, fold=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spZeuYqGPbOR"
      },
      "source": [
        "## read the tissue images & GTs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N63nSLT5O6za"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from patchify import patchify, unpatchify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJbxgUwmj0-Z"
      },
      "outputs": [],
      "source": [
        "print(cv2.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfyJ4CLUPBeV"
      },
      "outputs": [],
      "source": [
        "train_dir = \"dataset/monuseg/stain_normalized/train/tissue_images\"\n",
        "train_maskdir = \"dataset/monuseg/stain_normalized/train/binary_masks\"\n",
        "train_mask2dir = \"dataset/monuseg/stain_normalized/train/modified_masks\"\n",
        "\n",
        "val_dir   = \"dataset/monuseg/stain_normalized/validation/tissue_images\"\n",
        "val_maskdir = \"dataset/monuseg/stain_normalized/validation/binary_masks\"\n",
        "val_mask2dir = \"dataset/monuseg/stain_normalized/validation/modified_masks\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUDndE1dPV_c"
      },
      "outputs": [],
      "source": [
        "W = 1024\n",
        "H = 1024\n",
        "\n",
        "patch_size = (256, 256, 3)\n",
        "all_img_patches = []\n",
        "for x in tqdm(sorted(glob.glob(os.path.join(train_dir, \"*\"))), total=len(os.listdir(train_dir))):\n",
        "    single_img = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "    single_img = cv2.cvtColor(single_img, cv2.COLOR_BGR2RGB)\n",
        "    single_img = cv2.resize(single_img, (W, H), interpolation=cv2.INTER_LINEAR)\n",
        "    # patchify\n",
        "    single_img_patches = patchify(single_img, patch_size=patch_size, step=128)\n",
        "    # squeeze\n",
        "    single_img_patches = np.squeeze(single_img_patches)\n",
        "\n",
        "    for i in range(single_img_patches.shape[0]):\n",
        "        for j in range(single_img_patches.shape[1]):\n",
        "            all_img_patches.append(single_img_patches[i, j])\n",
        "    \n",
        "train_images = np.array(all_img_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2gfdNtxPkhw"
      },
      "outputs": [],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyCid9IcPlyK"
      },
      "outputs": [],
      "source": [
        "all_img_patches = []\n",
        "for x in tqdm(sorted(glob.glob(os.path.join(val_dir, \"*\"))), total=len(os.listdir(val_dir))):\n",
        "    single_img = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "    single_img = cv2.cvtColor(single_img, cv2.COLOR_BGR2RGB)\n",
        "    single_img = cv2.resize(single_img, (W, H), interpolation=cv2.INTER_LINEAR)\n",
        "    # patchify\n",
        "    single_img_patches = patchify(single_img, patch_size=patch_size, step=128)\n",
        "    # squeeze\n",
        "    single_img_patches = np.squeeze(single_img_patches)\n",
        "\n",
        "    for i in range(single_img_patches.shape[0]):\n",
        "        for j in range(single_img_patches.shape[1]):\n",
        "            all_img_patches.append(single_img_patches[i, j])\n",
        "\n",
        "val_images = np.array(all_img_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXX6QqKAP4Zi"
      },
      "outputs": [],
      "source": [
        "val_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVsn7JEIP5cI"
      },
      "outputs": [],
      "source": [
        "all_mask_patches = []\n",
        "for x in tqdm(sorted(glob.glob(os.path.join(train_maskdir, \"*\"))), total=len(os.listdir(train_maskdir))):\n",
        "    single_mask = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n",
        "    single_mask = cv2.resize(single_mask, (W, H), interpolation=cv2.INTER_NEAREST)\n",
        "    # patchify\n",
        "    single_mask_patches = patchify(single_mask, patch_size=(256, 256), step=128)\n",
        "    # squeeze\n",
        "    single_mask_patches = np.squeeze(single_mask_patches)\n",
        "\n",
        "    for i in range(single_mask_patches.shape[0]):\n",
        "        for j in range(single_mask_patches.shape[1]):\n",
        "            all_mask_patches.append(single_mask_patches[i, j])\n",
        "\n",
        "train_masks = np.array(all_mask_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43ERJyE_QH1V"
      },
      "outputs": [],
      "source": [
        "train_masks.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkOkN8NIQI5W"
      },
      "outputs": [],
      "source": [
        "all_mask_patches = []\n",
        "for x in tqdm(sorted(glob.glob(os.path.join(val_maskdir, \"*\"))), total=len(os.listdir(val_maskdir))):\n",
        "    single_mask = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n",
        "    single_mask = cv2.resize(single_mask, (W, H), interpolation=cv2.INTER_NEAREST)\n",
        "    # patchify\n",
        "    single_mask_patches = patchify(single_mask, patch_size=(256, 256), step=128)\n",
        "    # squeeze\n",
        "    single_mask_patches = np.squeeze(single_mask_patches)\n",
        "\n",
        "    for i in range(single_mask_patches.shape[0]):\n",
        "        for j in range(single_mask_patches.shape[1]):\n",
        "            all_mask_patches.append(single_mask_patches[i, j])\n",
        "\n",
        "val_masks = np.array(all_mask_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILL2tG8pQMD3"
      },
      "outputs": [],
      "source": [
        "val_masks.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-jFoL9jQM4l"
      },
      "outputs": [],
      "source": [
        "all_mask_patches = []\n",
        "for x in tqdm(sorted(glob.glob(os.path.join(train_mask2dir, \"*\"))), total=len(os.listdir(train_mask2dir))):\n",
        "    single_mask = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n",
        "    single_mask = cv2.resize(single_mask, (W, H), interpolation=cv2.INTER_NEAREST)\n",
        "    # patchify\n",
        "    single_mask_patches = patchify(single_mask, patch_size=(256, 256), step=128)\n",
        "    # squeeze\n",
        "    single_mask_patches = np.squeeze(single_mask_patches)\n",
        "\n",
        "    for i in range(single_mask_patches.shape[0]):\n",
        "        for j in range(single_mask_patches.shape[1]):\n",
        "            # mask_with_boarders = generate_boarder(single_mask_patches[i, j])\n",
        "            all_mask_patches.append(single_mask_patches[i, j])\n",
        "\n",
        "train_masks2 = np.array(all_mask_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrBDQrF3QPwd"
      },
      "outputs": [],
      "source": [
        "train_masks2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIgmGmtcQT1u"
      },
      "outputs": [],
      "source": [
        "all_mask_patches = []\n",
        "for x in tqdm(sorted(glob.glob(os.path.join(val_mask2dir, \"*\"))), total=len(os.listdir(val_mask2dir))):\n",
        "    single_mask = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n",
        "    single_mask = cv2.resize(single_mask, (W, H), interpolation=cv2.INTER_NEAREST)\n",
        "    # patchify\n",
        "    single_mask_patches = patchify(single_mask, patch_size=(256, 256), step=128)\n",
        "    # squeeze\n",
        "    single_mask_patches = np.squeeze(single_mask_patches)\n",
        "\n",
        "    for i in range(single_mask_patches.shape[0]):\n",
        "        for j in range(single_mask_patches.shape[1]):\n",
        "            # mask_with_boarders = generate_boarder(single_mask_patches[i, j])\n",
        "            all_mask_patches.append(single_mask_patches[i, j])\n",
        "\n",
        "val_masks2 = np.array(all_mask_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUSpkjxnQVzm"
      },
      "outputs": [],
      "source": [
        "val_masks2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Au_Cw7npQX-p"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "rnd = np.random.randint(len(train_images))\n",
        "# rnd = 222\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize=(12, 6))\n",
        "[axi.set_axis_off() for axi in ax.ravel()]\n",
        "\n",
        "ax[0].imshow(train_images[rnd])\n",
        "ax[0].set_title(\"Tissue Image\")\n",
        "\n",
        "ax[1].imshow(train_masks[rnd])\n",
        "ax[1].set_title(\"Mask\")\n",
        "\n",
        "# ax[2].imshow(train_images[rnd])\n",
        "ax[2].imshow(train_masks2[rnd])\n",
        "ax[2].set_title(\"Mask2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn5BQJI4QYuf"
      },
      "source": [
        "## One-hot encoding the modified masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVRjk1hjQjcw"
      },
      "outputs": [],
      "source": [
        "# train masks\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "n, h, w = train_masks.shape\n",
        "train_masks_reshaped = train_masks2.reshape(-1,)\n",
        "train_masks_reshaped_encoded = label_encoder.fit_transform(train_masks_reshaped)\n",
        "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
        "\n",
        "n_classes = 3\n",
        "train_masks_cat = to_categorical(train_masks_encoded_original_shape, num_classes=n_classes)\n",
        "train_masks_cat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09zsQ6T2QrDS"
      },
      "outputs": [],
      "source": [
        "# val masks\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "n, h, w = val_masks.shape\n",
        "val_masks_reshaped = val_masks2.reshape(-1,)\n",
        "val_masks_reshaped_encoded = label_encoder.fit_transform(val_masks_reshaped)\n",
        "val_masks_encoded_original_shape = val_masks_reshaped_encoded.reshape(n, h, w)\n",
        "\n",
        "n_classes = 3\n",
        "val_masks_cat = to_categorical(val_masks_encoded_original_shape, num_classes=n_classes)\n",
        "val_masks_cat.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVBMAy7EREoG"
      },
      "source": [
        "## Data augmentation using albumentations library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yUdZajtQvwz"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from keras.utils import Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_t-im4zkMDt"
      },
      "outputs": [],
      "source": [
        "print(A.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4s62aRPc5B8"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLKh8vZHRLpf"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, images, masks, masks_cat, augmentations=None, batch_size=8, img_size=256, n_channels=3, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "        self.masks_cat = masks_cat\n",
        "\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.n_channels = n_channels\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augmentations\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.ceil(len(self.images) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indices of the batch\n",
        "        indices = self.indices[index * self.batch_size: min((index + 1) * self.batch_size, len(self.images))]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.data_generation(indices)\n",
        "        y1 = y[0]\n",
        "        y2 = y[1]\n",
        "\n",
        "        if self.augment is None:\n",
        "            return X, [np.array(y1), np.array(y2)]\n",
        "        else:            \n",
        "            im, mask1, mask2 = [], [], []   \n",
        "            for x, y1, y2 in zip(X, y1, y2):\n",
        "                augmented = self.augment(image=x, mask1=y1, mask2=y2)\n",
        "                im.append(augmented['image'])\n",
        "                mask1.append(augmented['mask1'])\n",
        "                mask2.append(augmented['mask2'])\n",
        "\n",
        "            return np.array(im), [np.array(mask1), np.array(mask2)]\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indices after each epoch'\n",
        "        self.indices = np.arange(len(self.images))\n",
        "        if self.shuffle == True:\n",
        "            t = 1000 * time.time() # current time in milliseconds\n",
        "            np.random.seed(int(t) % 2**32)\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "    def data_generation(self, indices):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((len(indices), self.img_size, self.img_size, self.n_channels))\n",
        "        y1 = np.empty((len(indices), self.img_size, self.img_size, 1))\n",
        "        y2 = np.empty((len(indices), self.img_size, self.img_size, 3)) # 3 classes (Nuclei, Border, Background)\n",
        "        # Generate data\n",
        "        for n, i in enumerate(indices):\n",
        "            X[n] = self.images[i]\n",
        "            y1[n] = (self.masks[i]/255.)[..., np.newaxis]\n",
        "            y2[n] = self.masks_cat[i]\n",
        "\n",
        "        return np.uint8(X), [np.float32(y1), np.float32(y2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNrD-xJTRYqj"
      },
      "outputs": [],
      "source": [
        "AUGMENTATIONS_TRAIN = A.Compose([\n",
        "    A.Rotate(limit=360, p=0.5),\n",
        "    A.OneOf([\n",
        "        A.HorizontalFlip(),\n",
        "        A.VerticalFlip(),\n",
        "        ], p=0.5),\n",
        "    A.OneOf([\n",
        "        A.RandomBrightnessContrast(),\n",
        "        A.RandomGamma(),\n",
        "        A.GaussNoise()\n",
        "         ], p=0.3),\n",
        "    A.OneOf([\n",
        "        A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
        "        # A.ElasticTransform(alpha=3, sigma=150, alpha_affine=150),\n",
        "        # A.ElasticTransform(),\n",
        "        A.Affine(translate_percent=0.2, shear=30, mode=cv2.BORDER_CONSTANT),\n",
        "        A.GridDistortion(),\n",
        "        A.OpticalDistortion(distort_limit=2, shift_limit=0.5),\n",
        "        ], p=0.3),\n",
        "    A.OneOf([\n",
        "        A.RGBShift(r_shift_limit=40, g_shift_limit=40,  b_shift_limit=40),\n",
        "        A.ColorJitter(hue=0.1),\n",
        "        A.Blur(blur_limit=3)\n",
        "        ], p=0.3),\n",
        "    A.ToFloat(max_value=255)\n",
        "], p=1,\n",
        "additional_targets={'image': 'image', 'mask1': 'mask', 'mask2':'mask'})\n",
        "\n",
        "AUGMENTATIONS_VAL = A.Compose([\n",
        "    A.ToFloat(max_value=255)\n",
        "], p=1,\n",
        "additional_targets={'image': 'image', 'mask1': 'mask', 'mask2':'mask'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sahhqd3vRjCP"
      },
      "source": [
        "### Testing data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8-9L49JRf7C"
      },
      "outputs": [],
      "source": [
        "# Single tissue image with 256*256 tiles (50% overlap between tiles) without augmentation\n",
        "a = DataGenerator(train_images, train_masks, train_masks_cat, batch_size=49, augmentations=None, shuffle=False)\n",
        "images, masks = a.__getitem__(0)\n",
        "\n",
        "max_images = 49\n",
        "grid_width = 7\n",
        "grid_height = int(max_images / grid_width)\n",
        "fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*2, grid_height*2))\n",
        "\n",
        "for i,(im, mask1, mask2) in enumerate(zip(images, masks[0], masks[1])):\n",
        "    ax = axs[int(i / grid_width), i % grid_width]\n",
        "    ax.imshow(im)\n",
        "    # ax.imshow(mask1.squeeze(), alpha=0.4)\n",
        "    ax.imshow(mask2, alpha=0.4)\n",
        "    ax.axis('off')\n",
        "\n",
        "print(mask1.shape)\n",
        "print(mask2.shape)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ekm-jFneR4hn"
      },
      "outputs": [],
      "source": [
        "# Same tissue image with augmentations\n",
        "a = DataGenerator(train_images, train_masks, train_masks_cat, batch_size=49, augmentations=AUGMENTATIONS_TRAIN, shuffle=False)\n",
        "images, masks = a.__getitem__(0)\n",
        "\n",
        "max_images = 49\n",
        "grid_width = 7\n",
        "grid_height = int(max_images / grid_width)\n",
        "fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*2, grid_height*2))\n",
        "\n",
        "for i,(im, mask1, mask2) in enumerate(zip(images, masks[0], masks[1])):\n",
        "    ax = axs[int(i / grid_width), i % grid_width]\n",
        "    ax.imshow(im)\n",
        "    # ax.imshow(mask1.squeeze(), alpha=0.4)\n",
        "    ax.imshow(mask2, alpha=0.4)\n",
        "    ax.axis('off')\n",
        "\n",
        "print(mask1.shape)\n",
        "print(mask2.shape)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjqpdPNXTZ66"
      },
      "source": [
        "## Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fOCU6VMLkSIy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "print(tf.__version__)\n",
        "print(keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTHG5ruqLD_m"
      },
      "outputs": [],
      "source": [
        "# backbone = \"vgg19\"\n",
        "backbone = \"efficientnetb4\"\n",
        "input_shape = (256, 256, 3)\n",
        "model = sm.Unet(\n",
        "             backbone_name=backbone,\n",
        "             input_shape=input_shape,\n",
        "             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5McDn_dWU6I"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jC-eAphWUub"
      },
      "outputs": [],
      "source": [
        "from keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HpGYX_INWXgp"
      },
      "outputs": [],
      "source": [
        "plot_model(model, show_shapes=True, dpi=330, to_file=\"U-Net_DD.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ8oAgALaj8-"
      },
      "source": [
        "### Defining loss function for binary & multi segmentation tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "as9BTX7-Zpg0"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6w5pzGifnVz"
      },
      "outputs": [],
      "source": [
        "# https://github.com/yingkaisha/keras-unet-collection\n",
        "!pip install keras_unet_collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efXpJJ4QfjXK"
      },
      "outputs": [],
      "source": [
        "from keras_unet_collection import losses\n",
        "\n",
        "def binary_loss(y_true, y_pred):\n",
        "\n",
        "    loss_focal = losses.focal_tversky(y_true, y_pred, alpha=0.5, gamma=4/3)\n",
        "    loss_iou = losses.iou_seg(y_true, y_pred)\n",
        "    \n",
        "    # (x) \n",
        "    # loss_ssim = losses.ms_ssim(y_true, y_pred, max_val=1.0, filter_size=4)\n",
        "    \n",
        "    return loss_focal+loss_iou #+loss_ssim\n",
        "\n",
        "def multi_loss(y_true, y_pred):\n",
        "\n",
        "    loss_focal = losses.focal_tversky(y_true, y_pred, alpha=0.7, gamma=4/3)\n",
        "    loss_iou = losses.iou_seg(y_true, y_pred)\n",
        "    \n",
        "    # (x) \n",
        "    # loss_ssim = losses.ms_ssim(y_true, y_pred, max_val=1.0, filter_size=4)\n",
        "    \n",
        "    return loss_focal+loss_iou #+loss_ssim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUN_1pCkaDZW"
      },
      "outputs": [],
      "source": [
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ET3WNbwb3U-"
      },
      "source": [
        "### Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHRIkAr_bWjn"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=Adam(1e-4),\n",
        "              loss={\n",
        "                    \"Binary_segmentation\": binary_loss, \n",
        "                    \"MultiClass_segmentation\": multi_loss},\n",
        "              metrics={\n",
        "                       \"Binary_segmentation\": dice_coef, \n",
        "                       \"MultiClass_segmentation\": dice_coef})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NY7qhfHdjT9"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwcWk3exb1Dt"
      },
      "source": [
        "### Defining some useful callbacks "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADk25mhZcCsi"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, Callback, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "import datetime\n",
        "\n",
        "from skimage import filters\n",
        "from scipy.ndimage import measurements\n",
        "from skimage.segmentation import watershed, mark_boundaries\n",
        "\n",
        "# Visualize training \n",
        "class loss_history(Callback):\n",
        "\n",
        "    def __init__(self, x=4):\n",
        "        self.x = x\n",
        "    \n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        fig, ax = plt.subplots(1, 5, figsize=(18, 12))\n",
        "        [axi.set_axis_off() for axi in ax.ravel()]\n",
        "\n",
        "        ax[0].imshow(train_images[self.x])\n",
        "        ax[0].set_title(\"Tissue Image\")\n",
        "\n",
        "        ax[1].imshow(train_masks[self.x], cmap=\"gray\")\n",
        "        ax[1].set_title(\"Ground Truth\")\n",
        "\n",
        "        model_sample_input = train_images[self.x].astype(\"float32\") / 255.\n",
        "        preds_train1, preds_train2 = self.model.predict(np.expand_dims(model_sample_input, axis=0), verbose=0)\n",
        "        preds1 =  np.squeeze(preds_train1[0]) >= 0.5\n",
        "        ax[2].imshow(preds1, cmap=\"gray\")\n",
        "        ax[2].set_title(\"Nuclei prediction\")\n",
        "        preds2 = preds_train2[0][:, :, 2] - preds_train2[0][:, :, 1] >= 0.5\n",
        "        ax[3].imshow(preds2, cmap=\"gray\")\n",
        "        ax[3].set_title(\"Nuclei marker prediction\")\n",
        "        \n",
        "        grad = filters.scharr(preds1)\n",
        "        marker = preds1 * preds2\n",
        "        marker = measurements.label(marker)[0]\n",
        "        proced_pred = watershed(grad, marker, mask=preds1)\n",
        "        ax[4].imshow(mark_boundaries(train_images[self.x], proced_pred, color=(0, 0, 1)))\n",
        "        ax[4].set_title(\"Result\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "lr_scheduler = PolynomialDecay(\n",
        "    initial_learning_rate=1e-4,\n",
        "    end_learning_rate=1e-7,\n",
        "    decay_steps=40\n",
        ")\n",
        "\n",
        "# lr_scheduler = tf.keras.optimizers.schedules.CosineDecay(\n",
        "#     1e-4, 100, alpha=0.0, name=None\n",
        "# )\n",
        "\n",
        "class ADJUSTLR(keras.callbacks.Callback):\n",
        "    def __init__ (self, model, freq, factor, verbose):\n",
        "        self.model=model\n",
        "        self.freq=freq\n",
        "        self.factor =factor\n",
        "        self.verbose=verbose\n",
        "        self.adj_epoch=freq\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch + 1 == self.adj_epoch: # adjust the learning rate\n",
        "            lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
        "            new_lr=lr * self.factor\n",
        "            self.adj_epoch +=self.freq\n",
        "            if self.verbose == 1:\n",
        "                print('\\non epoch ',epoch + 1, ' lr was adjusted from ', lr, ' to ', new_lr)\n",
        "            tf.keras.backend.set_value(self.model.optimizer.lr, new_lr) # set the learning rate in the optimizer\n",
        "\n",
        "\n",
        "create_path(\"logs/Unet\")\n",
        "csv_log = CSVLogger('logs/Unet/Unet_log01.csv', separator=',')\n",
        "model_name = f\"Unet_v00_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        "create_path(\"models/Unet\")\n",
        "path_to_save_model = \"models/Unet/\" + model_name + \".h5\"\n",
        "# checkpointer = ModelCheckpoint(path_to_save_model, verbose=1, save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-7, verbose=1)\n",
        "# reduce_lr = LearningRateScheduler(schedule=lr_scheduler)\n",
        "early_stop   = EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True)\n",
        "\n",
        "print(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWlQhnxEdBDc"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "            #  loss_history(), \n",
        "            #  checkpointer, \n",
        "             reduce_lr, \n",
        "            #  csv_log, \n",
        "             early_stop\n",
        "             ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7X3dJTMdn5t"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "epochs = 120\n",
        "\n",
        "# Generators\n",
        "training_generator = DataGenerator(train_images, train_masks, train_masks_cat, augmentations=AUGMENTATIONS_TRAIN, batch_size=batch_size)\n",
        "validation_generator = DataGenerator(val_images, val_masks, val_masks_cat, augmentations=AUGMENTATIONS_VAL, batch_size=batch_size)\n",
        "\n",
        "history = model.fit(training_generator,\n",
        "                    validation_data=validation_generator,                        \n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_OYcrtwmjcE"
      },
      "outputs": [],
      "source": [
        "model.save(path_to_save_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}